# BuriedBrains: A Roguelike-Inspired Multi-Agent RL Environment

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.18079360.svg)](https://doi.org/10.5281/zenodo.18079360)

## üìú Vis√£o Geral

**BuriedBrains** √© um ambiente de simula√ß√£o procedural, parcialmente observ√°vel (POMDP) e de alto risco, projetado como um benchmark para pesquisa em Aprendizado por Refor√ßo (RL). O projeto evoluiu de um ambiente *Single-Agent* (Fase 1) para uma arquitetura **Multi-Agent (Fase 2)** completa, capaz de suportar intera√ß√µes sociais complexas, combate PvP e dilemas de coopera√ß√£o versus trai√ß√£o.

Inspirado em jogos do g√™nero roguelike, o ambiente utiliza mec√¢nicas como morte permanente (com respawn estrat√©gico), gera√ß√£o procedural de n√≠veis baseada em grafos e um sistema de reputa√ß√£o **Karma** persistente.

Este reposit√≥rio cont√©m a implementa√ß√£o completa do ambiente, os scripts de treinamento e as ferramentas de valida√ß√£o utilizadas no Trabalho de Conclus√£o de Curso (TCC) em Engenharia da Computa√ß√£o na Universidade Federal do Cear√° (UFC) - Campus Sobral.

## üéÆ Inspira√ß√£o

O projeto **BuriedBrains** foi inspirado diretamente no jogo mobile *Buriedbornes* (Nussygame). A simplicidade visual aliada √† profundidade estrat√©gica desse jogo serviu como base para criar um ambiente onde agentes devem gerenciar cooldowns, equipamentos e riscos. Na Fase 2, o projeto expande esse conceito introduzindo "Zonas de Encontro" (Santu√°rios), inspiradas em lobbies multiplayer e dilemas sociais da Teoria dos Jogos.

## ‚ú® Funcionalidades Principais

### Fase 1: Core PvE (Validado)
* **Gera√ß√£o Procedural via Grafos:** N√≠veis de progress√£o modelados como Grafos Ac√≠clicos Dirigidos (DAGs) com poda din√¢mica de ramos n√£o escolhidos.
* **Parcial Observabilidade (POMDP):** O agente opera com um vetor de observa√ß√£o limitado (38 estados), exigindo mem√≥ria (LSTM) para inferir contextos t√°ticos.
* **Combate T√°tico:** Sistema de turnos com skills, cooldowns, efeitos de status (Stun, DoT, Buffs) e escalonamento de dificuldade.
* **Sistema de Equipamentos:** Loot com raridade (Comum a Lend√°rio) e l√≥gica de decis√£o estrat√©gica para upgrades.

### Fase 2: Arquitetura Social & Multiagente (Implementada)
* **Estrutura MAE (Multi-Agent Environment):** O ambiente gerencia m√∫ltiplos agentes simult√¢neos com espa√ßos de a√ß√£o/observa√ß√£o independentes (`gym.spaces.Dict`).
* **M√°quina de Estados H√≠brida:** * `PROGRESSION`: Agentes exploram seus pr√≥prios mundos PvE isolados.
    * `ARENA_SYNC`: Mec√¢nica de sincroniza√ß√£o temporal para aguardar oponentes.
    * `ARENA_INTERACTION`: Transi√ß√£o para grafos c√≠clicos (`Erd≈ës-R√©nyi`) onde agentes interagem fisicamente.
* **Mec√¢nicas Sociais:**
    * **Barganha Inferida:** Detec√ß√£o de inten√ß√£o cooperativa atrav√©s de a√ß√µes de "Dropar/Pegar Artefato".
    * **Trai√ß√£o:** Detec√ß√£o de ataques ap√≥s ofertas de paz, com penalidades severas de Karma.
    * **Sistema de Karma:** Modelo de reputa√ß√£o persistente que sobrevive √† morte do agente, permitindo consequ√™ncias de longo prazo em jogos iterados.
    * **Morte e Respawn:** Agentes derrotados reiniciam sua progress√£o (N√≠vel 1), mas mant√™m sua identidade e hist√≥rico social (Karma).

## üéØ Objetivos Cient√≠ficos

Este projeto foi desenhado para testar hip√≥teses espec√≠ficas sobre Intelig√™ncia Artificial:

1. **Relev√¢ncia da Mem√≥ria (H1):** Validada. Experimentos demonstraram que agentes com mem√≥ria (LSTM) superam significativamente agentes reativos (PPO) em cen√°rios com chefes e mec√¢nicas temporais complexas.
2. **Tomada de Decis√£o sob Risco (H2):** Validada. Agentes aprendem a evitar a√ß√µes inv√°lidas e gerenciar equipamentos para maximizar a sobreviv√™ncia.
3. **Emerg√™ncia de Comportamento Social (H3):** Arquitetura implementada para permitir que estrat√©gias de coopera√ß√£o ou trai√ß√£o surjam organicamente em fun√ß√£o do Karma e do contexto (diferen√ßa de poder).

## üìä Resultados (Fase 1)

* **Valida√ß√£o do Ambiente:** O ambiente provou-se desafiador, com agentes "m√©dios" morrendo no mid-game (andares 100-150) devido ao escalonamento de dificuldade.
* **Mem√≥ria vs. Reativo:** Agentes LSTM demonstraram estabilidade de aprendizado (`explained_variance` ~0.7), enquanto agentes PPO sofreram colapso de pol√≠tica em cen√°rios complexos.
* **Estrat√©gia de Equipamento:** Logs comprovam que o agente aprendeu a comparar a raridade de itens no ch√£o com os equipados, realizando apenas trocas vantajosas.

## üõ†Ô∏è Tecnologias

* **Linguagem:** Python 3.x
* **Core RL:** Gymnasium, Stable Baselines3, SB3-Contrib (RecurrentPPO)
* **Otimiza√ß√£o:** Optuna (Hyperparameter Optimization)
* **Grafos:** NetworkX (Modelagem topol√≥gica de Dungeons e Arenas)
* **Configura√ß√£o:** PyYAML
* **Dados:** NumPy, PyTorch

## üöÄ Instala√ß√£o

1. **Clone o reposit√≥rio:**
    ```bash
    git clone [https://github.com/maelsilvatt/buriedbrains-roguelike-rl-env.git](https://github.com/maelsilvatt/buriedbrains-roguelike-rl-env.git)
    cd buriedbrains-roguelike-rl-env
    ```
2. **Crie o ambiente virtual:**
    ```bash
    python -m venv venv
    source venv/bin/activate  # Linux/macOS
    # ou
    .\venv\Scripts\activate  # Windows
    ```
3. **Instale as depend√™ncias:**
    ```bash
    pip install -r requirements.txt
    ```

## ‚ñ∂Ô∏è Uso (Treinamento)

O script `train.py` suporta treinamento de longa dura√ß√£o, checkpoints e continua√ß√£o de treino (resume).

```bash
python train.py [op√ß√µes]

```

**Op√ß√µes Principais:**

* `--total_timesteps <int>`: Total de passos de treino (ex: 5000000).
* `--max_episode_steps <int>`: Dura√ß√£o m√°xima do epis√≥dio/vida (recomendado: 50000 para Fase 2).
* `--budget_multiplier <float>`: Dificuldade do gerador de conte√∫do (1.0 = Normal).
* `--load_path <str>`: Caminho para um arquivo `.zip` de modelo para **continuar o treinamento**.
* `--suffix <str>`: Nome identificador da run (para logs e salvamento).

## üîÆ Pr√≥ximos Passos (Fase 3 - Experimentos Sociais)

Com a arquitetura MAE implementada no `env.py`, os pr√≥ximos passos da pesquisa envolvem:

* **Treinamento Self-Play:** Implementar um loop de treino customizado para alimentar a rede neural com as experi√™ncias de ambos os agentes (`a1`, `a2`) simultaneamente.
* **An√°lise de Karma:** Executar simula√ß√µes de longa dura√ß√£o para observar se o Karma acumulado influencia a taxa de agress√£o em encontros futuros (vingan√ßa/coopera√ß√£o).
* **Visualiza√ß√£o:** Conectar o simulador a uma interface gr√°fica (Unity) via sockets para demonstrar as intera√ß√µes em tempo real.

## üìÑ Cita√ß√£o

Se este software foi √∫til para sua pesquisa, por favor cite-o conforme abaixo:

```bibtex
@misc{silva2025buriedbrains,
  author       = {Silva, Ismael Soares da},
  title        = {BuriedBrains: Um Ambiente Multiagente Procedural e Parcialmente Observ√°vel para Benchmark de Mem√≥ria},
  year         = {2025},
  version      = {v2.3.2},
  publisher    = {Zenodo},
  doi          = {10.5281/zenodo.18079360},
  howpublished = {\url{[https://doi.org/10.5281/zenodo.18079360](https://doi.org/10.5281/zenodo.18079360)}},
  note         = {Trabalho de Conclus√£o de Curso (Engenharia da Computa√ß√£o) -- Universidade Federal do Cear√°, Campus Sobral. Orientador: Prof. Dr. Thiago Iachiley Ara√∫jo de Souza}
}

```

## ‚öñÔ∏è Licen√ßa

Este projeto √© licenciado sob a Licen√ßa MIT. Veja o arquivo `LICENSE` para mais detalhes.

```
